#!/usr/bin/env python3
import argparse
import googleapiclient.discovery
from google.oauth2 import service_account
import time
import sys
from plumbum import FG
import requests
from bs4 import BeautifulSoup
import re
import toml

# -----------
# GCP profile
# -----------
GCP_PROJECT_ID = "weaver-411410"
GCP_USERNAME = "mafaldacf"
GCP_CLOUD_STORAGE_BUCKET_NAME = "weaver-411410_cloudbuild"
# -----------------
# GCP configuration
# -----------------
DEV_INSTANCE_NAME = "weaver-dsb-socialnetwork"
DEV_IMAGE_NAME = "weaver-dsb-socialnetwork"
DEV_INSTANCE_HOSTNAME = "weaver-dsb-socialnetwork.dev"
DEV_ZONE = "europe-west3-a"
INSTANCE_NAME_MANAGER = "weaver-dsb-db-manager"
INSTANCE_NAME_EU = "weaver-dsb-db-eu"
INSTANCE_NAME_US = "weaver-dsb-db-us"
ZONE_MANAGER = "europe-west3-a"
ZONE_EU = "europe-west3-a"
ZONE_US = "us-central1-a"
SSH_KEY_PATH = f"/home/{GCP_USERNAME}/.ssh/google_compute_engine"
CREDENTIALS_PATH = "gcp/credentials.json"

INSTANCES = [ # name, zone
  [INSTANCE_NAME_MANAGER, ZONE_MANAGER],
  [INSTANCE_NAME_EU, ZONE_EU],
  [INSTANCE_NAME_US, ZONE_US],
]
# --------------------
# Docker configuration
# --------------------
DOCKER_IMAGES = [ # tag, dir
  ['mongodb-delayed:4.4.6', 'mongodb-delayed'],
  ['mongodb-setup:4.4.6', 'mongodb-setup/post-storage'],
  ['rabbitmq-setup:3.8', 'rabbitmq-setup/write-home-timeline'],
]
# ------------------------
# Service Weaver Constants
# ------------------------
WEAVER_DASHBOARD_PORT=40599

# --------------
# Dynamic Config
# ---------------
credentials = service_account.Credentials.from_service_account_file(CREDENTIALS_PATH)
compute = googleapiclient.discovery.build('compute', 'v1', credentials=credentials)

def gather_metrics(addr):
  dashboard_url = f"http://{addr}:{WEAVER_DASHBOARD_PORT}/"
  response = requests.get(dashboard_url)
  dashboard_content = response.text

  soup = BeautifulSoup(dashboard_content, 'html.parser')
  deployment_links = soup.find_all('a', href=lambda href: href and '/deployment?id=' in href)
  deployment_ids = [link['href'].split('=')[-1] for link in deployment_links]
  deployment_id = deployment_ids[0]

  metrics_url = f"http://{addr}:{WEAVER_DASHBOARD_PORT}/metrics?id={deployment_id}"
  response = requests.get(metrics_url)
  metrics_content = response.text
  
  values = re.findall(r'composed_posts\{.*?\}\s(\d+)', metrics_content)
  num_composed_posts = sum(int(value) for value in values)

  values = re.findall(r'inconsistencies\{.*?\}\s(\d+)', metrics_content)
  num_inconsistencies = sum(int(value) for value in values)

  pc_inconsistencies = "{:.2f}".format((num_inconsistencies / num_composed_posts) * 100)
  return num_composed_posts, num_inconsistencies, pc_inconsistencies

# --------------------
# GCP
# --------------------

def get_instance_ips(instance_name, zone):
  instance = compute.instances().get(project=GCP_PROJECT_ID, zone=zone, instance=instance_name).execute()
  network_interface = instance['networkInterfaces'][0]
  # public, private
  return network_interface['accessConfigs'][0]['natIP'], network_interface['networkIP']

def storage_start():
  from plumbum.cmd import gcloud

  # get public ip for each instance
  public_ip_manager, _ = get_instance_ips(INSTANCE_NAME_MANAGER, ZONE_MANAGER)
  public_ip_eu, _ = get_instance_ips(INSTANCE_NAME_EU, ZONE_EU)
  public_ip_us, _ = get_instance_ips(INSTANCE_NAME_US, ZONE_US)

  # --- swarm manager
  cmd = f'sudo docker swarm init --advertise-addr {public_ip_manager}:2377'
  gcloud['compute', 'ssh', INSTANCE_NAME_MANAGER, '--zone', ZONE_MANAGER, '--command', cmd] & FG

  cmd = f'sudo docker network create --attachable -d overlay deathstarbench_network'
  gcloud['compute', 'ssh', INSTANCE_NAME_MANAGER, '--zone', ZONE_MANAGER, '--command', cmd] & FG

  cmd = f'sudo docker swarm join-token --quiet worker > token.txt'
  gcloud['compute', 'ssh', INSTANCE_NAME_MANAGER, '--zone', ZONE_MANAGER, '--command', cmd] & FG

  gcloud['compute', 'scp', f"{INSTANCE_NAME_MANAGER}:token.txt", 'gcp/token.txt'] & FG

  f = open('gcp/token.txt', 'r')
  token = f.read().strip()
  f.close()

  # --- nodes
  cmd = f'sudo docker swarm join --token {token} {public_ip_manager}:2377 --advertise-addr {public_ip_eu}'
  gcloud['compute', 'ssh', INSTANCE_NAME_EU, '--zone', ZONE_EU, '--command', cmd] & FG

  cmd = f'sudo docker swarm join --token {token} {public_ip_manager}:2377 --advertise-addr {public_ip_us}'
  gcloud['compute', 'ssh', INSTANCE_NAME_US, '--zone', ZONE_US ,'--command', cmd] & FG

  # --- manager
  cmd = f'sudo docker stack deploy --with-registry-auth --compose-file ~/weaver-dsb-socialnetwork/docker-compose.yml socialnetwork'
  gcloud['compute', 'ssh', INSTANCE_NAME_MANAGER, '--command', cmd] & FG

def storage_info():
  from plumbum.cmd import gcloud
  cmd = f'sudo docker node ls'
  gcloud['compute', 'ssh', INSTANCE_NAME_MANAGER, '--command', cmd] & FG
  cmd = f'sudo docker service ls'
  gcloud['compute', 'ssh', INSTANCE_NAME_MANAGER, '--command', cmd] & FG

  print()
  public_ip_manager, _ = get_instance_ips(INSTANCE_NAME_MANAGER, ZONE_MANAGER)
  print("storage manager running at @", public_ip_manager)
  public_ip_eu, _ = get_instance_ips(INSTANCE_NAME_EU, ZONE_EU)
  print(f"storage in {ZONE_EU} running at @", public_ip_eu)
  public_ip_us, _ = get_instance_ips(INSTANCE_NAME_US, ZONE_US)
  print(f"storage in {ZONE_US} running at @", public_ip_us)
  print()

  gen_weaver_config(public_ip_eu, public_ip_us)

def gen_weaver_config(public_ip_eu = "0.0.0.0", public_ip_us = "0.0.0.0"):
  data = toml.load("weaver.toml")

  for _, config in data.items():
    for entry, values in config.items():
      if entry in ['mongodb_address', 'redis_address', 'rabbitmq_address', 'memcached_address']:
        values["europe-west3"] = public_ip_eu
        values["us-central-1"] = public_ip_us

  f = open("weaver-gcp.toml",'w')
  toml.dump(data, f)
  f.close()
  print("[INFO] generated app config at weaver-gcp.toml")

def storage_deploy():
  from plumbum.cmd import terraform, mkdir, cp, gsutil, rm

  mkdir['weaver-dsb-socialnetwork'] & FG
  cp['-r', 'docker-compose.yml', 'docker', 'config', 'weaver-dsb-socialnetwork'] & FG
  gsutil['cp', '-r', 'weaver-dsb-socialnetwork', f'gs://{GCP_CLOUD_STORAGE_BUCKET_NAME}/'] & FG
  rm['-r', 'weaver-dsb-socialnetwork'] & FG

  terraform['-chdir=./terraform', 'init'] & FG
  terraform['-chdir=./terraform', 'apply'] & FG

  print("[INFO] waiting 120 seconds to install all dependencies in GCP instances...")
  time.sleep(120)
  
def storage_clean():
  from plumbum.cmd import terraform
  terraform['-chdir=./terraform', 'destroy'] & FG

def metrics():
  #FIXME
  #from plumbum.cmd import weaver
  #weaver['multi', 'status'] & FG


  num_composed_posts, num_inconsistencies, pc_inconsistencies = gather_metrics("localhost")
  print("# composed posts:\t", num_composed_posts)
  print("# inconsistencies:\t", num_inconsistencies)
  print("% inconsistencies:\t", pc_inconsistencies)

def wkr2():
  pass

# --------------------
# LOCAL
# --------------------

def local_storage_deploy():
  pass

def local_storage_start():
  pass

def local_metrics():
  num_composed_posts, num_inconsistencies, pc_inconsistencies = gather_metrics("localhost")
  print("# composed posts:\t", num_composed_posts)
  print("# inconsistencies:\t", num_inconsistencies)
  print("% inconsistencies:\t", pc_inconsistencies)

def local_storage_clean():
  pass

if __name__ == "__main__":
  main_parser = argparse.ArgumentParser()
  command_parser = main_parser.add_subparsers(help='commands', dest='command')

  commands = [
    # ----------------
    # gcp datastores
    'storage-deploy', 'storage-start', 'storage-info', 'storage-clean', 
    # gcp app
    'wrk2', 'metrics',
    # ----------------
    # local datastores
    'local-storage-deploy', 'local-storage-start', 'local-storage-clean'
    # local app
    'local-wrk2', 'local-metrics',
  ]
  for cmd in commands:
      command_parser.add_parser(cmd)
  args = vars(main_parser.parse_args())
  command = args.pop('command').replace('-', '_')

  print(f"[INFO] ----- {command.upper()} -----")
  getattr(sys.modules[__name__], command)(**args)
  print(f"[INFO] done!")
